{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc1155f6-8fda-447c-a0db-1dd9179ef3c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Risposta: 1. In Big Data ecosystems, managing data quality, governance, and scalability are major challenges. Data comes from diverse sources, requiring integration and validation.\n",
      "2. Balancing real-time processing with batch analysis involves prioritizing use cases and leveraging appropriate technologies.\n",
      "3. Data lake, hybrid systems, and data warehouses each offer unique advantages for performance and flexibility.\n",
      "4. Machine learning and data streaming technologies can be integrated through pipelines, ensuring resilience and maintainability.\n",
      "5.\n",
      "Chunk usati: [{'chunk': 'Questo può essere un punto di conflitto per alcuni sviluppatori. Esiste una classe importante di ingegneri che gestiscono le massicce infrastrutture distribuite necessarie per archiviare e analizzare, ad esempio, le transazioni finanziarie o i dati dei social media. su un livello di scala pari a quello di Facebook o Twitter. Infatti, dedicherò il Capitolo 12 alle sfide distintive delle infrastrutture di big data. Questi ingegneri stanno costruendo strumenti e sistemi per supportare la scienza dei dati, anche se potrebbero non estrarre personalmente i dati che gestiscono.', 'score': 0.6219896249150925}, {'chunk': \"L'approccio basato sui big data potrebbe analizzare i feed massicci di Twitter o Facebook, interpretando gli indizi delle loro opinioni nel testo. L'approccio basato sui piccoli dati potrebbe essere quello di condurre un sondaggio, ponendo a qualche centinaio di persone questa domanda specifica e tabulando i risultati. Quale procedura pensa che si rivelerà più accurata? Il set di dati giusto è quello più direttamente rilevante per i compiti da svolgere, non necessariamente il più grande. Non aspiri ciecamente ad analizzare grandi serie di dati.\", 'score': 0.6074878897760239}, {'chunk': 'Nella scienza tradizionale guidata dalle ipotesi, progettiamo un esperimento per raccogliere esattamente i dati di cui abbiamo bisogno per rispondere alla nostra domanda specifica. Ma i big data sono più tipicamente il prodotto di un processo di registrazione di eventi discreti, o forse di contributi distribuiti da milioni di persone sui social media. I dati Lo scienziato in genere ha un controllo minimo o nullo del processo di raccolta, solo un vago incarico di trasformare tutti quei bit in denaro.', 'score': 0.5727660210770539}]\n",
      "Tempo di esecuzione: 5.23 secondi\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time  # Importa il modulo time\n",
    "\n",
    "# URL del tuo endpoint FastAPI\n",
    "url = \"http://localhost:8000/ask\"\n",
    "\n",
    "# Payload da inviare\n",
    "# \"question\": \"cosa sono i big data?\" ok / 4.47s\n",
    "# \"question\": \"Che cosa sono le reti indotte?\" ok / 5.37s\n",
    "# \"question\": \"Come faccio a valutare un modello?\", ok / 5s\n",
    "# \"question\": \"Descrivi il Teorema dell'impossibilità di Arrow\" metà giusta / 6s\n",
    "# \"question\": \"Quali sono le 5 v dei big data?\" ok / 5,56s aggiornamenti con nuovo modello\n",
    "# \"question\":\"Cos'è un modello di classificazione?\" ok ma mancano spazi / 5.37s\n",
    "# \"question\"Cos'è la divergenza di Kullback-Leibler?\" ok /6.22s\n",
    "# Quali sono le principali differenze tra dati strutturati, semi-strutturati e non strutturati nel contesto dei Big Data? ok/5.30\n",
    "# Nel contesto dei Big Data, come possono le aziende garantire l'affidabilità e la qualità dei dati raccolti da fonti eterogenee come social media, sensori IoT e log di sistema? Quali sono le principali sfide legate alla pulizia, all'integrazione e alla validazione di questi dati, e in che modo le moderne pipeline di data processing affrontano questi problemi?\n",
    "# In un ecosistema Big Data moderno, le aziende si trovano a raccogliere, archiviare e analizzare enormi volumi di dati provenienti da fonti eterogenee come dispositivi IoT, social media, log applicativi, transazioni finanziarie e molto altro. In questo contesto, quali sono le principali sfide legate alla gestione della qualità del dato, alla governance e alla scalabilità dei sistemi? Come si bilancia la necessità di elaborazione in tempo reale con quella di analisi batch più approfondite? Inoltre, quali strumenti e architetture (come data lake, data warehouse o sistemi ibridi) risultano più efficaci per garantire prestazioni elevate e flessibilità nel lungo termine? Infine, come si integrano tecnologie come il machine learning o il data streaming (es. Kafka, Flink) all'interno di pipeline dati resilienti e manutenibili, e quale ruolo ha il data engineer nella progettazione di questi flussi complessi?\n",
    "\n",
    "payload = {\n",
    "    \"question\": \"\"\n",
    "}\n",
    "\n",
    "# Disabilitiamo il proxy per le richieste a localhost\n",
    "proxies = {\n",
    "    \"http\": None,\n",
    "    \"https\": None,\n",
    "}\n",
    "\n",
    "# Calcoliamo il tempo di inizio\n",
    "start_time = time.time()\n",
    "\n",
    "# Eseguiamo la POST\n",
    "response = requests.post(url, json=payload, proxies=proxies)\n",
    "\n",
    "# Calcoliamo il tempo totale\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "# Controlliamo e stampiamo il risultato\n",
    "if response.ok:\n",
    "    data = response.json()\n",
    "    print(\"Risposta:\", data.get(\"answer\"))\n",
    "    print(\"Chunk usati:\", data.get(\"chunks_used\"))\n",
    "else:\n",
    "    print(f\"Errore: {response.status_code}\", response.text)\n",
    "\n",
    "# Stampiamo il tempo di esecuzione\n",
    "print(f\"Tempo di esecuzione: {elapsed_time:.2f} secondi\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bac6fa-fbaa-4e2b-9c94-f24f8fdca5e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Transformers (Python 3.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
